TL;DR:Â Weâ€™ve released instrumentation packages for theÂ claude-agent-sdkÂ in bothÂ PythonÂ andÂ TypeScript for developers to monitor and debug their custom Claude agents. Both SDKs use a tiny, transparentÂ Rust proxy to capture every prompt, tool call, and latency metric from the Claude Code process locally and efficiently.

With the release of theÂ Claude Agent SDK, you aren't limited to using Claude as a terminal helper. You can now buildÂ custom coding agents, like automated PR reviewers, self-healing CI/CD pipelines, or bespoke coding assistants integrated directly into your product.

However, the SDK has a seriousÂ ObservabilityÂ problem. When you embed Claude Code into your Python or Node application, it becomes a black box of subprocesses. You need to know:

Did my agent fail because of my application logic or the LLM response?

What exact prompt did the SDK send to the underlying Claude process?

Which tools (file edits, bash commands) were actually executed?

Out of the box, this isn't possible.

AtÂ Laminar, we obsess over developer experience. We wanted to ensure that if you are building an agent on top of Claude, you can trace the entire execution flow, from your outer function to the inner workings of the Claude Node process, with zero friction.

The industry standard for this depth of analysis usually demands complex, multi-step workflows that drag researchers away from their actual work. We aggressively engineered our solution to maintain the simple initialisation and minimal footprint that we strive for.

The Challenge

Last week, agent developers asked us to instrumentÂ the SDK so that they could go through their Laminar workflows while building up custom agents. They wanted to see:

What was sent to the LLM.

How much time every call took.

What tools were called.

How this linked to their outer flows calling the SDK.

The difficult part is that the SDK spins up aÂ Node processÂ with the actual Claude Code, which is effectively a separate environment. We needed a way to bridge the gap between your application code and that isolated process.

Our requirements were:

âœ… Instrument Claude agent SDK functions for the outside trace structure.

ğŸ¤” Catch and instrument the LLM calls from the Claude Code Node process.

âŒÂ Most importantly:Â Make sure that data traced in step 1 and step 2 is under the same trace.

Attempt 1: The LiteLLM Proxy

The Claude Agent SDK allows changingÂ ANTHROPIC_BASE_URL. Since Laminar has a great integration withÂ LiteLLM already, we thought we could point the SDK to a central LiteLLM proxy.

The architecture for launchingÂ claude-agent-sdkÂ many times looked like this:

The Setup:Â Multiple Claude Code processes; one central proxy (because running a full FastAPI/Flask server for every process is impractical).

The Flow:Â Python or Node processes send structure spans (likeÂ connect()) to our backend. The LiteLLM proxy sends LLM data (prompts, tool results) to the same backend.

The Problem: Correlation

While the structure spans containÂ trace IDs, the LLM spans containÂ different trace IDs.Â How do we associate the two?

We considered spinning up a side endpoint on the proxy to sayÂ "Request ABC belongs to Trace 123,"Â but this created too much overhead:

Users would have to change their LiteLLM proxy code.

We would need to pass metadata down to the actual LLM requests.

Our backend would have to parse this extra metadata.

Attempt 2: Native Claude Code Logs

Claude Code already sends logs that areÂ OTEL compatible!

We reasoned that if we setÂ OTEL_EXPORTER_OTLP_LOGS_ENDPOINTÂ andÂ OTEL_EXPORTER_OTLP_HEADERS, we could relay metadata to our backend to associate logs with spans.

We spun up an endpoint to ingest these logs and convert them to our span format.

The Problem: Missing Data & Immutable Environment

Missing Data:Â Claude Code only sends metadata (duration, token counts, errors).Â No prompt data is logged (unless it's the user prompt, which is opt-in).

Process Control:Â claude-agent-sdkÂ reconnects to existing processes. If the Node process is already running, we cannot modify the environment variables (like trace headers) of that running process from the outside.

The Solution: A Lightweight Rust Proxy

We needed something controllable, non-intrusive, and invokable from Python & Node. Since Python and "lightweight" are rarely synonymous and Node event loops are easily blocked, we turned toÂ Rust.

We designed a small Rust server invoked from Python & Node usingÂ PyO3 & NAPI-RS bindings.

Why this works:

No Central Proxy:Â Unlike the LiteLLM solution, you don't need a centralized server.

Local & Fast:Â The proxy is lightweight and lives close to the actual Claude Code project.

Performance:Â It re-streams response tokens back and processes observed data via tokio::spawn in Rust, making latency impact negligible.

Portable:Â The binary (packaged Python wheel or Node native add-on for TypeScript) is under 1.5MB and runs on almost any platform.

Zero-Friction Setup:Â The solution works out of the box with just installation and initializationâ€”no configuration of environment variables or base URLs is required.

The Result

By solving the process-boundary problem with our Rust proxy, Laminar now provides the only drop-in solution for tracingÂ claude-agent-sdk.

Traces now collect all LLM prompts, tool call inputs, and tool outputs. Crucially, the actual LLM calls are properly nested under your application's query span.

True to our obsession with developer experience, we refused to make this complicated. The setup is just an install command and 1-2 lines of code. We respect the time of the researchers and engineers who rely on Laminar. We want you analyzing your agents, not wrestling with your instrumentation.

You can view a sample trace here

Option A: Python

Installation

Bash

pip install lmnr[claude-agent-sdk]
# or
uv add lmnr[claude-agent-sdk]
export LMNR_PROJECT_API_KEY=YOUR_PROJECT_KEY

Usage

Python

import asyncio

from claude_agent_sdk import ClaudeSDKClient
from lmnr import Laminar, observe

Laminar.initialize()

@observe()
async def main():
    async with ClaudeSDKClient() as client:
        await client.query(
            "Explain to me with examples, how memoization speeds up recursive "
            "function calls. Use the Fibonacci sequence as an example."
        )
        async for msg in client.receive_response():
            print(msg)

if __name__ == "__main__":
    asyncio.run(main())

Option B: TypeScript / JavaScript

InstallationÂ Ensure you are usingÂ @lmnr-ai/lmnrÂ versionÂ 0.7.10Â or higher.

Bash

npm install @lmnr-ai/lmnr @anthropic-ai/claude-agent-sdk
# or
pnpm add @lmnr-ai/lmnr @anthropic-ai/claude-agent-sdk
export LMNR_PROJECT_API_KEY=YOUR_PROJECT_KEY

UsageÂ In the JS SDK, you simply wrap the query function.

TypeScript

import { query as origQuery } from "@anthropic-ai/claude-agent-sdk";
import { Laminar } from "@lmnr-ai/lmnr";
// 1. Initialize Laminar
Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY
});
// 2. Wrap the original query function
const query = Laminar.wrapClaudeAgentQuery(origQuery);
async function run() {
  // 3. Use the wrapped query function
  const result = await query({
    prompt: "Scan the current directory for TODOs and create a summary markdown file."
  });
  
  console.log(result);
}
run();

For more details, check theÂ Documentation.