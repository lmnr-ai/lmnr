---
title: "Introducing Signals"
date: "2026-01-26"
description: "Extract insights from any AI agent trace, past or present. Define a signal once, backfill your history, and catch failures before users do."
author:
  name: Sam Komesarook
  url: https://x.com/raymondmarkable
tags: ["signals", "events", "tracing", "analysis", "llm-observability", "agent-debugging"]
---

Observability tells you what happened. Signals tell you what it means.

When you're debugging agent behavior at scale, raw traces aren't enough. You need to extract structured insights: Did the agent hallucinate? Did it call the wrong tool? Did latency spike because of a specific retrieval pattern? Signals let you define these questions once, then answer them across every trace you've ever collected.

![Signals events and clusters view](/blog/2026-01-26-signals-page.png)

## What is a signal?

A signal is a definition: a name, a prompt, and a structured output schema. Once defined, Laminar runs it against trace context with an LLM to produce structured signal events.

Signals complement code-emitted events. Keep your explicit instrumentation, and add signals when you need to extract meaning from traces.

This means you can define signals for behaviors you only discovered after the fact: hallucinations in a specific workflow, unexpected tool calls, or latency patterns that correlate with poor outputs.

![Create signal sheet with prompt, schema, and test trace](/blog/2026-01-26-create-new.png)

## Backfill first, live second

Signals aren't tied to live traffic. Run a signal over existing traces, validate it, then decide whether it should run continuously.

The workflow:

1. **Test** the signal on a known trace to verify output.
2. **Backfill** a time range or filtered slice of your trace history. Select thousands of traces using filters and run them as a background job. We built dedicated infrastructure for this, using Gemini's batch API for large-scale processing.
3. **Trigger** on new traces to keep it running. Triggers support custom filters: match traces by span name, trace status, and more. We're adding filters based on feedback, like "trace duration exceeds 100s."

![Add trigger dialog with custom filters](/blog/2026-01-26-add-trigger.png)

Every signal run is logged. You can see exactly which runs produced events and which didn't.

Root cause analysis becomes something you can do retroactively, not just in real time. The traces you collected last month are now as useful as the ones coming in today.

![Signal jobs list showing backfill progress](/blog/2026-01-26-run-jobs.png)

## From signal to event to action

When a signal runs against a trace, it produces an event: a structured payload with a timestamp. Once you have events, you can:

- **Cluster** similar events to surface patterns and regressions
- **Export** into datasets for evaluations
- **Track** volume over time and watch for shifts

Clustering is where this gets powerful. Instead of reading traces one by one, you see groups of similar failures. A spike in one cluster tells you exactly where to focus.

## Why this matters

Signals make agent behavior searchable and repeatable. Mine your historical traces for answers, then keep those same definitions running in production to catch future changes.

The goal: spend less time reading traces, more time acting on what's inside them.

Try Signals today and let us know what you think.
